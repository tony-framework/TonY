import java.time.Duration

apply plugin: "java"
apply plugin: "eclipse"
apply plugin: "idea"

sourceCompatibility = 1.8

buildscript {
  repositories {
    jcenter()
    maven {
      url "https://plugins.gradle.org/m2/"
    }
  }
  dependencies {
    classpath "com.github.jengelman.gradle.plugins:shadow:2.0.4"
    classpath "com.google.protobuf:protobuf-gradle-plugin:0.8.1"
    classpath "gradle.plugin.nl.javadude.gradle.plugins:license-gradle-plugin:0.14.0"
    classpath "io.codearte.gradle.nexus:gradle-nexus-staging-plugin:0.20.0"
    classpath "de.marcphilipp.gradle:nexus-publish-plugin:0.4.0"
  }
}

apply plugin: 'io.codearte.nexus-staging'
nexusStaging {
  packageGroup = "com.linkedin.tony"
  stagingProfileId = "6f8a92e81628a4"
  username = findProperty('mavenUser')
  password = findProperty('mavenPassword')
  delayBetweenRetriesInMillis = 3000
  numberOfRetries = 50
}


def hadoopVersion = "2.10.0"
ext.playVersion = "2.6.20"
ext.scalaVersionFull = System.getProperty("scala.binary.version", "2.11.12")
ext.scalaVersion = scalaVersionFull
if (ext.scalaVersionFull.split("\\.").length == 3) {
  ext.scalaVersion = ext.scalaVersionFull.substring(0, ext.scalaVersionFull.lastIndexOf('.'))
}

ext.deps = [
  azkaban: [
    "az_core": "com.linkedin.azkaban:az-core:3.58.0",
    "az_hadoop_jobtype_plugin": "com.linkedin.azkaban:az-hadoop-jobtype-plugin:3.58.0",
    "azkaban_common": "com.linkedin.azkaban:azkaban-common:3.58.0",
    "azkaban_hadoop_security_plugin": "com.linkedin.azkaban:azkaban-hadoop-security-plugin:3.58.0"
  ],
  hadoop: [
    "common": "org.apache.hadoop:hadoop-common:${hadoopVersion}",
    "common_test": "org.apache.hadoop:hadoop-common:${hadoopVersion}:tests",
    "hdfs": "org.apache.hadoop:hadoop-hdfs:${hadoopVersion}",
    "hdfs_test": "org.apache.hadoop:hadoop-hdfs:${hadoopVersion}:tests",
    "mapreduce_client_core": "org.apache.hadoop:hadoop-mapreduce-client-core:${hadoopVersion}",
    "mapreduce_client_hs": "org.apache.hadoop:hadoop-mapreduce-client-hs:${hadoopVersion}",
    "mapreduce_client_common": "org.apache.hadoop:hadoop-mapreduce-client-common:${hadoopVersion}",
    "mapreduce_client_app": "org.apache.hadoop:hadoop-mapreduce-client-app:${hadoopVersion}",
    "minicluster": "org.apache.hadoop:hadoop-minicluster:${hadoopVersion}",
    "yarn_api": "org.apache.hadoop:hadoop-yarn-api:${hadoopVersion}",
    "yarn_client": "org.apache.hadoop:hadoop-yarn-client:${hadoopVersion}",
    "yarn_common": "org.apache.hadoop:hadoop-yarn-common:${hadoopVersion}",
    "yarn_server_test": "org.apache.hadoop:hadoop-yarn-server-tests:${hadoopVersion}:tests"
  ],
  external: [
    "assertj": "org.assertj:assertj-core:3.6.2",
    "avro": "org.apache.avro:avro:1.8.2",
    "awaitility": "org.awaitility:awaitility:2.0.0",
    "commons_io": "commons-io:commons-io:2.4",
    "guava": "com.google.guava:guava:11.0.2",
    "guice": "com.google.inject:guice:3.0",
    "httpclient": "org.apache.httpcomponents:httpclient:4.5.3",
    "jackson_databind": "com.fasterxml.jackson.core:jackson-databind:2.8.3",
    "jackson_dataformat_yaml": "com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.9.6",
    // Only needed by Hadoop test classes
    "junit": "junit:junit:4.12",
    "log4j": "log4j:log4j:1.2.17",
    "metrics": "com.codahale.metrics:metrics-core:3.0.2",
    "mockito": "org.mockito:mockito-core:2.23.0",
    "objenesis": "org.objenesis:objenesis:2.6",
    "play_guice": "com.typesafe.play:play-guice_$scalaVersion:$playVersion",
    "play_logback": "com.typesafe.play:play-logback_$scalaVersion:$playVersion",
    "scala_compiler": "org.scala-lang:scala-compiler:$scalaVersionFull",
    "slf4j_api": "org.slf4j:slf4j-api:1.7.25",
    "sshd": "org.apache.sshd:sshd-core:1.1.0",
    "testng": "org.testng:testng:6.4",
    "text": "org.apache.commons:commons-text:1.4",
    "zip4j": "net.lingala.zip4j:zip4j:1.3.2"
  ]
]

allprojects {
  group = "com.linkedin.tony"
  project.version = "0.4.4"
}

task sourcesJar(type: Jar) {
  from sourceSets.main.allJava
  classifier = 'sources'
}

task javadocJar(type: Jar) {
  from javadoc
  classifier = 'javadoc'
}

subprojects {
  apply plugin: "license"
  apply plugin: "com.google.protobuf"
  apply plugin: "java"
  apply plugin: "checkstyle"
  apply plugin: "findbugs"
  apply plugin: "eclipse"
  apply plugin: "idea"
  apply plugin: "maven-publish"
  apply plugin: "signing"
  apply plugin: 'de.marcphilipp.nexus-publish'

  apply from: "$rootDir/gradle/version-info.gradle"

  if (project.name.equals("tony-cli")) {
    apply plugin: 'com.github.johnrengelman.shadow'
    shadowJar {
      mergeServiceFiles()
      dependencies {
        exclude(dependency('com.google.guava:'))
        exclude(dependency('com.sun.jersey:'))
        exclude(dependency('com.sun.jersey.contribs:'))
        exclude(dependency('org.apache.hadoop:'))
        exclude(dependency('org.apache.hive:'))
        exclude(dependency('org.apache.pig:'))
        exclude(dependency('org.apache.spark:'))
        exclude(dependency('org.spark-project:'))
      }
      zip64 true
      classifier "uber"
    }

    build.dependsOn(shadowJar)
  }

  repositories {
    mavenCentral()
  }
  plugins.withType(JavaPlugin) {
    sourceCompatibility = 1.8
    dependencies {
      // dependency defined in product_spec.json
      testCompile deps.external.testng
    }

    test {
      useTestNG()
    }
  }

  license {
    header rootProject.file('license_header')
    // Set the year in the license
    ext.year = Calendar.getInstance().get(Calendar.YEAR)
    skipExistingHeaders = true
    excludes(
        ["com/linkedin/tony/rpc/proto/", "**/*.properties", "**/*.txt", "**/*.json", "**/*.jhist", "**/*.xml"])
  }
  configurations {
    hadoopRuntime.extendsFrom(runtime)
    hadoopRuntime {
      exclude group: "org.apache.hadoop"
    }
  }

  publishing {
    publications {
      mavenJava(MavenPublication) {
        artifactId = "${project.name}"
        if (project.name.equals("tony-portal")) {
          artifact source: "$buildDir/distributions/${project.name}-${project.version}.zip", builtBy: { tasks.createPlayBinaryZipDist }
        } else {
          from components.java
          artifact sourcesJar
          artifact javadocJar
        }
        if (project.name.equals("tony-cli")) {
          artifact shadowJar
        }
        pom {
          name = "${project.name}".toString()
          description = "${project.name}".toString()
          url = 'https://github.com/linkedin/TonY'
          licenses {
            license {
              name = "BSD 2-Clause"
              url = "https://opensource.org/licenses/BSD-2-Clause"
            }
          }
          developers {
            developer {
              id = 'erwa'
              name = 'Anthony Hsu'
            }
            developer {
              id = 'jhung'
              name = 'Jonathan Hung'
            }
            developer {
              id = 'oliverhu'
              name = 'Keqiu Hu'
            }
          }
          scm {
            connection = 'https://github.com/linkedin/TonY.git'
            developerConnection = 'git@github.com:linkedin/TonY.git'
            url = 'https://github.com/linkedin/TonY'
          }
        }
      }
    }
  }
  nexusPublishing {
    repositories {
      sonatype() {
        username = project.findProperty('mavenUser')
        password = project.findProperty('mavenPassword')
      }
    }
    clientTimeout = Duration.ofMinutes(5)
    connectTimeout = Duration.ofMinutes(5)
  }
  signing {
    sign publishing.publications.mavenJava
  }

  checkstyle {
    configDir rootProject.file('ligradle/checkstyle')
    configFile rootProject.file(new File(configDir.path, 'linkedin-checkstyle.xml'))
    maxWarnings 0
  }

  tasks.withType(FindBugs) {
    reports {
      xml.enabled false
      html.enabled true
    }

    classpath = classpath.filter {
      // POM files are getting included in the classpath for some reason, but this causes parsing errors when FindBugs
      // tries to analyze them, thinking they are zips. Excluding POM files as a workaround.
      !it.name.endsWith('.pom')
    }

    excludeFilter = rootProject.file('ligradle/findbugs/findbugsExclude.xml')
  }
}

apply plugin: 'distribution'

// Generates a closure which is used to set up the contents
// for a distribution; parametrized by the name of the
// configuration to include in the lib directory.
def generateDistContents(configurationName) {
  return {
    into('.') {
      from rootProject.fileTree('.') {
        include 'README.md'
        include 'LICENSE'
        include 'NOTICE'
        include 'CONTRIBUTING.md'
      }
    }
    into('bin') {
      def bashFiles = []
      rootProject.subprojects.each {
        bashFiles << it.fileTree("src/main/bash") {
          include "*.sh"
        }
      }
      from bashFiles
    }
    into('lib') {
      def dependencies = files()
      def jars = []
      rootProject.subprojects.each {
        // Use subtraction to eliminate duplicates
        dependencies = dependencies + (it.configurations[configurationName] - dependencies)
        jars << it.jar
      }
      from dependencies
      from jars
    }
  }
}

distributions {
  // main distribution does not include Hadoop JARs; this is the one
  // typically expected to be used on a system properly set up with
  // an existing Hadoop installation.
  main {
    baseName = rootProject.name
    contents generateDistContents('hadoopRuntime')
  }
  // fat distribution includes all dependencies.
  fat {
    baseName = rootProject.name + '-fat'
    contents generateDistContents('runtime')
  }
}
if (project.hasProperty('overrideBuildEnvironment')) {
  apply from: project.overrideBuildEnvironment
}

build.dependsOn(distZip)
